#!/usr/bin/env python3
import os
import sys

# Get the absolute path of this script
script_path = os.path.abspath(__file__)
script_dir = os.path.dirname(script_path)
main_dir = os.path.abspath(os.path.join(script_dir, "../.."))
os.chdir(main_dir)
sys.path.insert(0, main_dir)
print(f"Current Working Directory: {os.getcwd()}")

import additionals.mysql_functions
import openai
import json
import additionals.logger
import additionals.funcs

# Configuration Variables
API_KEY = ""  # Replace with your actual API key
MODEL_NAME = "gpt-4o-mini-realtime-preview"
PRODUCTS = []
MAX_TOKENS = 4096  # Ensure model can accept a lot of tokens
MIN_CVSS_SCORE = 8
PROMPT = """Retrieve CVEs from the past 24 hours with a CVSS score above min-cvss-score_to_check_in_prompt that are relevant to the following devices:
Products: products_to_check_in_prompt
Return the data in JSON format with a maximum of 3 CVEs per response. If there are more than 3 CVEs available, prepend true before the JSON; otherwise, prepend false.

Each CVE entry should have the following fields:

CVE ID – The identifier of the CVE
CVSS Score – The severity rating
Affected Device – The impacted system
YARA Rule – A YARA rule for detecting the vulnerability
Sigma Rule – A Sigma rule for detecting the vulnerability
Mitigation Tips – Recommended actions to mitigate the vulnerability
Generate YARA and Sigma rules specific to each CVE. Return only the JSON without additional explanations."""

JSON_OUTPUT_PATH = os.path.join("response_folder", "CVE_AI_MANAGMENT_RESULTS.json") # Default path, can be overridden

current_cve_index = 0
all_cves = []
client = ""

def setup_global_variables(config, logger):
    global API_KEY, MODEL, MAX_TOKENS, PROMPT, PRODUCTS, JSON_OUTPUT_PATH, client
    try:
        API_KEY = config.get('ClientData', {}).get('API', {}).get('AiManagement', '')
        if not API_KEY:
            logger.warning("API key not found in configuration")
            
        ai_config = config.get("General", {}).get("IntervalConfigurations", {}).get("AiManagement", {})
        if not ai_config:
            logger.warning("AiManagement configuration not found")
            ai_config = {}
            
        if ai_config.get("MODEL"):
            MODEL = ai_config["MODEL"]
        if ai_config.get("MAX_TOKENS"):
            MAX_TOKENS = ai_config["MAX_TOKENS"]
        if ai_config.get("MIN_CVSS_SCORE"):
            MIN_CVSS_SCORE = ai_config["MIN_CVSS_SCORE"]
        if ai_config.get("PROMPT"):
            PROMPT = ai_config["PROMPT"]
        if ai_config.get("PRODUCTS"):
            PRODUCTS = ai_config["PRODUCTS"]

        # Safely handle string replacements
        safe_min_cvss = str(MIN_CVSS_SCORE)
        safe_products = ", ".join(PRODUCTS) if PRODUCTS else "No products specified"
        
        PROMPT = PROMPT.replace("min-cvss-score_to_check_in_prompt", safe_min_cvss)
        PROMPT = PROMPT.replace("products_to_check_in_prompt", safe_products)
        
        logger.info("Prompt:\n" + PROMPT)
        
        # Initialize the client with a valid API key
        if API_KEY:
            client = openai.OpenAI(api_key=API_KEY)
            # Fetch list of available models
            #models = client.models.list()

            # Print all available model names
            #for model in models:
             #   logger.info(model.id)
        else:
            logger.error("Cannot initialize OpenAI client: API key is missing")
            
    except Exception as e:
        logger.error(f"Error in setup_global_variables: {str(e)}")
        # Set some defaults to prevent crashes
        if not API_KEY:
            logger.critical("No API key available - functionality will be limited")
        if not client:
            logger.critical("OpenAI client not initialized")


def query_gpt(logger):
    """Send prompt to GPT and retrieve the CVEs with logging."""
    
    if not client:
        logger.error("OpenAI client not initialized. Cannot query GPT.")
        return None
        
    try:
        logger.info("Sending query to GPT model...")

        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "You are an expert in cybersecurity providing CVE details."},
                {"role": "user", "content": PROMPT}
            ],
            max_tokens=4096
        )

        # Ensure the response is valid
        if not response or not response.choices:
            logger.error("GPT response is empty or malformed.")
            return None

        # Extract response text safely
        response_text = response.choices[0].message.content.strip() if response.choices and hasattr(response.choices[0], 'message') else ""
        
        if not response_text:
            logger.error("Empty response text from GPT")
            return None

        # Log the full GPT response
        logger.info(f"FULL GPT Response:\n{response_text}")

        return response_text

    except openai.APIConnectionError:
        logger.error("Failed to connect to OpenAI API. Check internet connection.")
        return None

    except openai.OpenAIError as e:
        logger.error(f"OpenAI API error: {str(e)}")
        return None

    except Exception as e:
        logger.error(f"Unexpected error in query_gpt: {str(e)}")
        return None


def parse_response(response_text, logger):
    """Parse the response and extract CVEs."""
    global all_cves
    
    if not response_text:
        logger.error("Empty response text received in parse_response")
        return False
        
    try:
        response_text = response_text.strip()
        
        if response_text.startswith("true"):
            json_data = response_text[4:].strip()
            prepend_true = True
        elif response_text.startswith("false"):
            json_data = response_text[5:].strip()
            prepend_true = False
        else:
            # Handle case where response doesn't have expected prefix
            logger.warning("Response doesn't start with 'true' or 'false', attempting to parse as JSON directly")
            json_data = response_text
            prepend_true = False
        
        # Make sure we have some JSON data to parse
        if not json_data:
            logger.error("No JSON data found after prefix")
            return False
            
        # Try to validate JSON before parsing
        try:
            # Check if JSON is valid by attempting to parse it first
            json.loads(json_data)
        except json.JSONDecodeError as e:
            # Try to clean the JSON data if possible
            logger.warning(f"Initial JSON validation failed: {str(e)}")
            
            # Look for JSON array brackets
            if '[' in json_data and ']' in json_data:
                start_idx = json_data.find('[')
                end_idx = json_data.rfind(']') + 1
                if start_idx >= 0 and end_idx > start_idx:
                    logger.info("Attempting to extract JSON array from response")
                    json_data = json_data[start_idx:end_idx]
            
        # Now try to parse the possibly cleaned JSON
        try:
            cve_list = json.loads(json_data)
            
            # Validate that we have a list
            if not isinstance(cve_list, list):
                logger.error("JSON data is not a list")
                return False
                
            # Check if each item has the required fields
            required_fields = ["CVE ID", "CVSS Score", "Affected Device"]
            valid_cves = []
            
            for cve in cve_list:
                if not isinstance(cve, dict):
                    logger.warning(f"Skipping non-dictionary CVE entry: {cve}")
                    continue
                    
                # Check if all required fields are present
                missing_fields = [field for field in required_fields if field not in cve]
                if missing_fields:
                    logger.warning(f"CVE entry missing required fields {missing_fields}: {cve}")
                    continue
                    
                valid_cves.append(cve)
                
            if not valid_cves:
                logger.error("No valid CVE entries found in response")
                return False
                
            all_cves.extend(valid_cves)
            return prepend_true
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response: {str(e)}")
            logger.debug(f"Problematic JSON data: {json_data}")
            return False
            
    except Exception as e:
        logger.error(f"Unexpected error in parse_response: {str(e)}")
        return False


def get_next_cves(logger):
    """Retrieve the next batch of CVEs."""
    global current_cve_index, all_cves
    
    try:
        # Check if we have CVEs in our existing cache
        if current_cve_index < len(all_cves):
            next_cves = all_cves[current_cve_index:current_cve_index + 3]
            current_cve_index += len(next_cves)
            prepend_true = current_cve_index < len(all_cves)
            return prepend_true, json.dumps(next_cves, indent=4)
        
        # If we're here, we need to fetch more CVEs
        response_text = query_gpt(logger)
        if not response_text:
            logger.error("Failed to get response from GPT")
            return False, json.dumps([])
        
        # Parse the new response
        more_available = parse_response(response_text, logger)
        
        # Get the newly added CVEs
        if current_cve_index < len(all_cves):
            next_cves = all_cves[current_cve_index:current_cve_index + 3]
            current_cve_index += len(next_cves)
            return more_available, json.dumps(next_cves, indent=4)
        else:
            # No new CVEs were added
            logger.info("No more CVEs available")
            return False, json.dumps([])
    except Exception as e:
        logger.error(f"Unexpected error in get_next_cves: {str(e)}")
        return False, json.dumps([])


def save_json_to_file(cve_data, logger):
    """Save the CVE data to a JSON file, combining with existing data if the file exists."""
    try:
        # Ensure we have valid JSON data
        if not cve_data:
            logger.error("Empty CVE data received, nothing to save")
            return False
            
        # Parse the current JSON data
        try:
            new_cves = json.loads(cve_data)
            
            # Validate that new_cves is a list
            if not isinstance(new_cves, list):
                logger.error(f"CVE data is not a list: {type(new_cves)}")
                return False
                
            # If new_cves is empty, no need to save
            if not new_cves:
                logger.info("No new CVEs to save")
                return True
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse CVE JSON data: {str(e)}")
            return False
        
        # Ensure output directory exists
        output_dir = os.path.dirname(JSON_OUTPUT_PATH)
        if output_dir and not os.path.exists(output_dir):
            logger.info(f"Creating output directory: {output_dir}")
            os.makedirs(output_dir, exist_ok=True)
        
        # Check if file exists and has content
        combined_data = []
        if os.path.exists(JSON_OUTPUT_PATH) and os.path.getsize(JSON_OUTPUT_PATH) > 0:
            logger.info(f"JSON file exists at {JSON_OUTPUT_PATH}, combining with existing data...")
            try:
                with open(JSON_OUTPUT_PATH, 'r') as file:
                    existing_data = json.load(file)
                    
                # Ensure existing_data is a list
                if not isinstance(existing_data, list):
                    logger.warning(f"Existing data is not a list: {type(existing_data)}")
                    existing_data = []
                    
                # Combine new data with existing data (new data at the top)
                combined_data = new_cves + existing_data
            except json.JSONDecodeError as e:
                logger.error(f"Error parsing existing JSON file: {str(e)}")
                # Continue with just the new data
                combined_data = new_cves
            except Exception as e:
                logger.error(f"Error reading existing JSON file: {str(e)}")
                # Continue with just the new data
                combined_data = new_cves
        else:
            logger.info(f"Creating new JSON file at {JSON_OUTPUT_PATH}")
            combined_data = new_cves
        
        # Write the combined data back to the file
        try:
            with open(JSON_OUTPUT_PATH, 'w') as file:
                json.dump(combined_data, file, indent=4)
                
            logger.info(f"Successfully saved data to {JSON_OUTPUT_PATH}")
            return True
        except Exception as e:
            logger.error(f"Error writing to JSON file: {str(e)}")
            return False
            
    except Exception as e:
        logger.error(f"Error saving JSON data: {str(e)}")
        return False


if __name__ == "__main__":
    try:
        logger = additionals.logger.setup_logger("alerts_vuln_cve_managment_helper.log")
        
        try:
            env_dict = additionals.funcs.read_env_file(logger)
        except Exception as e:
            logger.error(f"Error reading environment file: {str(e)}")
            env_dict = {}
            
        try:
            connection = additionals.mysql_functions.setup_mysql_connection(env_dict, logger)
            query_result = additionals.mysql_functions.execute_query(connection, "SELECT config FROM configjson LIMIT 1", logger)
            
            if query_result and query_result[0] and query_result[0][0]:
                config_data = json.loads(query_result[0][0])
            else:
                logger.error("No configuration data retrieved from database")
                config_data = {}
        except Exception as e:
            logger.error(f"Error connecting to database or retrieving config: {str(e)}")
            config_data = {}
            
        setup_global_variables(config_data, logger)
        
        # Check for command line argument to set output path
        if len(sys.argv) > 1:
            JSON_OUTPUT_PATH = sys.argv[1]
            logger.info(f"Output path set from command line: {JSON_OUTPUT_PATH}")
        
        prepend_true, cve_json = get_next_cves(logger)
        logger.info("true" if prepend_true else "false")
        logger.info("Full CVE before save:" + str(cve_json))
        
        # Save JSON data to file
        save_result = save_json_to_file(cve_json, logger)
        if save_result:
            logger.info(f"CVE data successfully saved to {JSON_OUTPUT_PATH}")
        else:
            logger.error(f"Failed to save CVE data to {JSON_OUTPUT_PATH}")
            
    except Exception as e:
        # Get global logger if it exists, otherwise create a basic one
        try:
            if 'logger' not in locals() or logger is None:
                import logging
                logging.basicConfig(level=logging.ERROR)
                logger = logging.getLogger("emergency_logger")
        except:
            pass
            
        try:
            logger.critical(f"Critical error in main execution: {str(e)}")
        except:
            print(f"CRITICAL ERROR: {str(e)}")